export async function handleDemoRequest(req, res, requestPurpose) {

  let responseContent = "example response content";
  
  if (requestPurpose == "Chat Submission" && req.body.stream) 
  {
    responseContent = "This is a demo response for a streaming model. No actual AI LLM Provider API was called. " +
    "You need to deploy your own instance of this app and provide OpenAI and Anthropic API keys to make it work. " +
    "Please refer to README.md for details."

    if (req.headers['x-model-provider']=='anthropic') {
        responseContent += '\n\nAnthropic models selection in the UI can be disabled by rebuilding the Docker image with an env variable VITE_ANTHROPIC_ENABLE=N';
    }
  }


    if (requestPurpose == "Chat Submission" && !req.body.stream) 
        responseContent = "This is a demo response for a non-streaming model like OpenAI o1. " +
        "No actual AI LLM Provider API was called. " +
        "You need to deploy your own instance of this app and provide OpenAI API key to make it work. " +
        "Please refer to README.md for details. " +
        "API access to o1 models may not be available at all API tiers. " +
        "It is possible disable selection of o1 models in the UI if you rebuild the Docker image with an env variable VITE_O1_MODELS_ENABLED=N";

    if (requestPurpose == "Title Generation") 
        responseContent = "Demo Title - Autogenerated";

  if (req.body.stream) {
    /* Streaming Mode */
    res.writeHead(200, {
      'Content-Type': 'text/event-stream',
      'Cache-Control': 'no-cache',
      'Connection': 'keep-alive',
    });

    const chunks = responseContent.split(' ');
    for (const chunk of chunks) {
      const streamChunk = { content: chunk + ' ' };
      res.write(`data: ${JSON.stringify(streamChunk)}\n\n`);
      await new Promise(resolve => setTimeout(resolve, 50)); // Simulate delay between chunks
    }

    res.write('data: [DONE]\n\n');
    res.end();
  } else {
    /* Batch Mode */
    const formattedBatchResponse = {
        message: {
          role: 'assistant',
          content: responseContent
        },
        usage: {
          prompt_tokens: 0,
          completion_tokens: 48,
          reasoning_tokens: 128
        }
      };

    console.log('Demo Batch Response:', JSON.stringify(formattedBatchResponse, null, 2));

    res.status(200).json(formattedBatchResponse);
  }

  return;
}
