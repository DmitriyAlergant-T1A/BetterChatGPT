{
  "configuration": "Конфигурация Модели Чата",
  "model": "Модель",
  "maxGenerationTokens": {
    "label": "Макс. токенов генерации",
    "description": "Максимальное количество токенов для генерации ответов. Инструктирует модель какой максимальной длины ответ мы разрешаем генерировать. Для \"рассуждающих\" моделей это также включает токены использованные во внутреннем рассуждении."
  },
  "maxPromptTokens": {
    "label": "Макс. токенов входных",
    "description": "Максимальное количество токенов отправляемых в модель, включая ваше последнее сообщение, системный промпт, и историю предыдущих сообщений (сколько поместиться). Когда чат становится слишком длинным, часть ранних сообщений не поместившаяся в этот лимит не будет включена в контекст."
  },
  "default": "По умолчанию",
  "temperature": {
    "label": "Температура",
    "description": "Значение температуры выборки от 0 до 2. Более высокие значения, например 0.8, сделают выходной результат более случайным, в то время как более низкие значения, например 0.2, более фокусированными и детерминированными. Параметр не используется для моделей o1-mini и o1-preview."
  },
  "presencePenalty": {
    "label": "Штраф за присутствие",
    "description": "Число от -2.0 до 2.0. Положительные значения штрафуют новые токены на основе их появления в тексте до этого момента, увеличивая вероятность перехода модели к новым темам. (По умолчанию: 0)"
  },
  "topP": {
    "label": "Top-p",
    "description": "Число от 0 до 1. Альтернатива выборке с температурой, называемая выборка ядра, при которой модель учитывает результаты токенов с верхним p вероятностных масс. Так, значение 0.1 означает, что рассматриваются только токены, составляющие верхние 10% вероятностной массы. Мы обычно рекомендуем изменять Top-p или температуру, но не оба. (По умолчанию: 1)"
  },
  "frequencyPenalty": {
    "label": "Штраф за частоту",
    "description": "Число от -2.0 до 2.0. Положительные значения штрафуют новые токены на основе их имеющейся частоты в тексте на данный момент, уменьшая вероятность повторения той же строки дословно. (По умолчанию: 0)"
  },
  "defaultChatConfig": "Конфигурация чата по умолчанию",
  "defaultSystemMessage": "Системное сообщение",
  "resetToDefault": "Восстановить значения по умолчанию",
  "newChatSelectModel": "Новый чат: Выберите модель",
  "warningActiveChat": "Внимание: текущий активный чат будет удален и заменен новым чатом.",
  "seeDropActiveChat": "См. переключатель \"Удалять активный чат при создании нового чата\" в настройках.",
  "smallerClass": "Эконом-класс: более дешевые и быстрые модели, но все еще весьма мощные.",
  "leadingFrontier": "Передовые модели. Лучший ИИ для продуктивного каждодневного использования.",
  "experimentalHeavy": "Экспериментальные вычислительно-тяжелые модели для сложных логических и научных задач.",
  "traditionalLLM": "Традиционные модели LLM",
  "traditionalLLMDescription": "Опираются на механизм внимания к контекстному окну и предобученные знания о мире, для немедленного ответа слово-за-словом. Рафинированный интеллект.",
  "reasoningIterative": "Модели рассуждения и итерации",
  "reasoningIterativeDescription": "\"Рассуждающая\" модель думает перед ответом. Она использует свою базовую LLM (4o-mini или 4o) для планирования многоступенчатого процесса мышления, затем рассуждает \"вслух\" (но тихо - внутри) и проводит несколько циклов рассуждения, пока сама не будет удовлетворена и готова ответить.",
  "confirmationTitle": "Подтверждение",
  "choiceConfirmationPrompt": "{{prompt}}"
}
